---
title: "Critical Points and Convergence Analysis of Generative Deep Linear
Networks Trained with Bures-Wasserstein Loss"
collection: publications
permalink: /publications/BPAM2023Critical.pdf
excerpt: 'We consider a deep matrix factorization model of covariance matrices
trained with the Bures-Wasserstein distance. While recent works have made
advances in the study of the optimization problem for overparametrized low-rank
matrix approximation, much emphasis has been placed on discriminative settings
and the square loss. In contrast, our model considers another type of loss and
connects with the generative setting. We characterize the critical points and
minimizers of the Bures-Wasserstein distance over the space of rank-bounded
matrices. The Hessian of this loss at low-rank matrices can theoretically blow
up, which creates challenges to analyze convergence of gradient optimization
methods. We establish convergence results for gradient flow using a smooth
perturbative version of the loss as well as convergence results for finite step
size gradient descent under certain assumptions on the initial weights.'
date: 2023
venue: 'ICML 2023'
paperurl: 'https://arxiv.org/abs/2303.03027'
---
This paper is about the number 1. The number 2 is left for future work.

[Download paper here](http://academicpages.github.io/files/paper1.pdf)

Recommended citation: Your Name, You. (2009). "Paper Title Number 1." <i>Journal 1</i>. 1(1).
